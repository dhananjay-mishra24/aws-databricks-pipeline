# ğŸ› ï¸ CSV to Parquet Data Pipeline using AWS, Databricks & PySpark

This is a beginner-friendly end-to-end data engineering project where I built a pipeline to ingest, clean, transform, and query COVID-19 data using **AWS S3**, **Databricks (Community Edition)**, **PySpark**, and **Athena**.

---

## ğŸš€ Project Overview

- âœ… Ingested the **Our World in Data COVID-19 dataset (CSV)** into **Amazon S3**
- ğŸ§¹ Cleaned missing values using **PySpark**:
  - Categorical columns (`string`) â†’ filled with `"Unknown"`
  - Numeric columns (`int`, `double`) â†’ filled with the **mean**
- ğŸ”„ Converted the dataset to **Parquet format**
- ğŸ’¾ Saved the transformed data back to **S3**
- ğŸ” Queried the Parquet data using **AWS Athena**

---

## ğŸ§° Tools & Technologies

- **AWS S3** â€“ Storage for raw CSV and Parquet files  
- **AWS IAM** â€“ Secure programmatic access for Databricks  
- **Databricks Community Edition** â€“ Spark-powered data processing  
- **PySpark** â€“ Distributed data cleaning and transformation  
- **Athena** â€“ SQL querying directly on Parquet in S3

---

## ğŸ“ Folder Structure

```text
raw/
â””â”€â”€ owid-covid-data.csv           # Original dataset

clean/
â””â”€â”€ owid-covid-parquet/           # Cleaned parquet output

athena-results/
â””â”€â”€ ...                           # Athena query outputs
# aws-databricks-pipeline
